\section{Spectral Element Method}
\subsection{Introduction}
The problem with the Finite Element Method is that the rate of convergence is limited by the degree of the polynomials used. An alternative can be the Spectral Element Method, for which the convergence rate is limited by the regularity of the solution. 
\subsection{Legendre polynomials}
The Legendre polynomials \(\{L_k(x) \in \mathbb{P}_k, k = 0, 1, \ldots\}\) are the eigenfunctions of the singular Sturm-Liouville problem:
\[
    ((1-x^2)L'_k(x))' + k(k+1)L_k(x) = 0 \quad -1 < x < 1
\]
So they satisfy the recurrence relation
\begin{equation}
    \begin{split}
        &L_0(x) = 1, \ L_1(x) = x, \txt{ and for } k \geq 1 \\
        &L_{k+1}(x) = \frac{2k+1}{k+1}xL_k(x) - \frac{k}{k+1}L_{k-1}(x)
    \end{split}
    \label{legendre_polynomials}
\end{equation}
Given a weight function \(w(x) \equiv 1\), they are mutually orthogonal with respect to it on the interval \((-1, 1)\)
\[
    \int_{-1}^1 L_k(x)L_m(x) \; dx = \begin{cases}
        \frac{2}{2k+1} & \txt{if } k = m \\
        0 & \txt{if } k \neq m
    \end{cases}
\]
The expansion of \(u \in L^2(-1,1)\) in terms of \(L_k\) is 
\[
    u(x) = \sum_{k=0}^{\infty} \hat{u}_k L_(x)
\]
Given that \((f,g) = \int_{-1}^1 fg \, dx\) we know that:
\[
    (u,L_m) = \sum_{k=0} \hat{u}_k (L_k, L_m) \underset{\txt{orth.}}{=} \hat{u}_m\frac{2}{2m+1} \Rightarrow \hat{u}_k = \frac{2k+1}{2} \int_{-1}^1 u L_k \, dx
\]
The truncated Legendre series of \(u\) is the \(L^2-\txt{projection of } u \txt{ over } \mathbb{P}_N\) is 
\begin{equation}
    P_Nu = \sum_{k=0}^{N} \hat{u}_k L_k
\end{equation}
Given any \(u \in H^s(-1,1)\) with \(s \in N\), the projection error \((u-P_Nu)\) satisfies the estimates 
\begin{align*}
    \norm{u-P_Nu}_{L^2(-1,1)} &\leq CN^{-s} \norm{u}_{H^s(-1,1)} & \forall \; s \geq 0 \\
    \norm{u-P_Nu}_{L^2(-1,1)} &\leq CN^{-s} \seminorm{u}{H^s(-1,1)} & \forall \; s \leq N+1 \\
\end{align*}
There is also a ``modified'' Legendre basis for function that vanish at \(\pm 1\). This is because the Legendre basis is not suited to impose Dirichlet B.C.
\begin{align}
    \psi_0(x) = \frac{1}{2} (L_0(x) - L_1(x)) = \frac{1-x}{2} \\ 
    \psi_N(x) = \frac{1}{2} (L_0(x) + L_1(x)) = \frac{1+x}{2} \\ 
    \psi_{k-1}(x) = \frac{1}{\sqrt{2(2k-1)}} (L_{k-2}(x) - L_k(x))\\ 
    \txt{for } k = 2, \ldots, N \ -1 < x < 1 \\ 
\end{align}

\subsection{Spectral Galerkin formulation}
Given \(\Omega = (-1, 1), \mu, b, \sigma > 0\) const., \(f: \Omega \to \real\). Look for \(u:\Omega \to \real\) s.t. 
\[
    \begin{cases}
        -(\mu u')'+(bu)' + \sigma u = f & \txt{in } \Omega \\
        u(-1) = 0 \\
        u(1) = 0
    \end{cases}
\]
Set \(V  = H^1_0(\Omega)\), then the weak form of the differential problem reads: 
\[
    \txt{find } u \in V \txt{ s.t } a(u,v) = (f,v)_{L^2(\Omega)} \quad \forall \; v \in V, \ f \in L^2(\Omega)
\]
where 
\begin{align*}
    & a(u,v) = \int_{\Omega} (\mu u' - bu)v'\, dx + \int_{\Omega} \sigma uv \, dx \\
    & (f,v)_{L^"(\Omega)} = \int_{\Omega} f v \, dx
\end{align*}
Now set \(V_N = \mathbb{P}^0_N\) 
\begin{equation}
    \txt{find } u_N \in V_N: a(u_N, v_N) = (f, v_N)_{L^2(\Omega)} \label{weak_spec_galerkin_formulation}
\end{equation}
Now expand \(u_N(x) = \sum_{k=1}^{N-1} \tilde{u}_k \psi_k(x)\) and chose \(v_N = \psi_i(x)\) for any \(i = 1, \ldots, N-1\).
The discretization of the problem reads:
\[
    \txt{find } u = \left[\tilde{u}\right]_{k=1}^{N-1} : \sum_{k=1}^{N-1} a(\phi_k, \psi_i)\tilde{u}_k = (f, \psi_i)_{L^2(\Omega)} \quad \txt{for any } i = 1, \ldots, N-1
\]
Given \(u_N \in V_N\) the solution of the problem, then if \(u \in H^{s+1}(\Omega)\) with \(s \geq 0\), thanks to CeÃ  Lemma, holds that:
\[
    \honenorm{u-u_N} \leq C(s)\left(\frac{1}{N}\right)^s\norm{u}_{H^{s+1}(\Omega)}
\]
So \(u_N\) converges with spectral accuracy with respect to \(N\).
But doing so we would have two full matrices, the stiffness one and the mass one  \(M_{ij} = (\psi_j \psi_i)_{L^2(-1,1)}\) are quite expensive to compute or invert.

To solve this we can use a Lagrange nodal basis instead of a modal one, by using the Legendre-Gauss-Lobatto quadrature formulas.
In this case we need a Legendre polynomial \(L_N(x)\).

Given a \(L_N(x)\) polynomial, we can put one node at each end of the domain, so \(x_0 = -1, x_N =  1\) and \(x_j = \txt{ zeros of }L'
_N\) with \(j =  1, \ldots, N-1\). We also need a set of weights \(w_j = \frac{2}{N(N+1)} \frac{1}{[L_N(x_j)]^2}\) with \(j = 0, \ldots, N\).

With this set of nodes and weights it's possible to obtain the following interpolatory quadrature formula
\[
    \int_{-1}^1 f(x) \, dx \approx \sum_{j=0}^{N} f(x_j)*w_j
\]
The degree of exactness of this method is \(2N-1\), meaning that 
\[
    \int_{-1}^1 f(x)\,dx = \sum_{j=0}^{N} f(x_j)w_j \quad \forall \; f \in \mathbb{P}_{2N-1}
\]
Some useful operation with LGL nodes
\begin{itemize}
    \item Discrete inner product in \(L^2(-1,1)\):
    \[
        (u,v)_N = \sum_{j=0}^{N} u(x_j)v(x_j)w_j 
    \]
    with degree of exactness \(2N-1\) 
    \[
        (u,v)_{L^2(\Omega)} = (u,v)_N \quad \txt{only if } u, v \in \mathbb{P}_{2N-1}
    \]
    \item Discrete norm in \(L^2(-1,1)\)
    \[
        \norm{u}_N = (u,u)_N^{\frac{1}{2}}
    \]
    with the following norm equivalence: \(\exists \; c_1, c_2 > 0\) s.t.
    \[
        c_1 \norm{v_N}_{L^2(-1,1)} \leq \norm{v_N}_N \leq c_2 \norm{v_N}_{L^2(-1,1)} \quad \forall \; v_N \in \mathbb{P}_N
    \]
\end{itemize}
Given \(\left\{\phi_0, \ldots, \phi_N\right\}\) characteristics Lagrange polynomials in \(\mathbb{P}_N\) w.r.t the LGL nodes. then
\[
    \phi_j = \frac{1}{n(n+1)} \frac{(1-x^2)}{(x_j-x)}\frac{L'_N(x)}{L_N(x_j)} \quad \txt{for } j = 0, \ldots, N
\]
Also true that \(\phi_j(x_k) = \delta_{kj}\) and \(\left\{\phi_j\right\}\) are orthogonal w.r.t. the discrete inner product \((\cdot,\cdot)_N\), meaning that the mass matrix \(M\) is diagonal. Given \(\left\{w_i\right\}\) the set of weights, then 
\[
    M_{ij} = (\phi_j, \phi_i)_N = \delta_{ij}w_i  \quad i,j  = 0, \ldots, N
\]
\subsection{Galerkin with Numerical Integration}
We can now define the spectral Galerkin method with numerical integration (GNI), by setting \(a_N(u_N, v_N) = (\mu u'_N-b u_n, v'_N)_N+(\sigma u_n, v_n)_N\), and the problem as
\[
    \txt{find } u_N^{\txt{GNI}} \in V_N : a_N(u_N^{\txt{GNI}},v_N)=(f, v_N)_N \quad \forall \; v_N \in V_N
\]
Then, by the same expansion w.r.t. the Lagrange basis: \(u_N^{\txt{GNI}}(x) =  \sum_{i=0}^{N}u_N^{\txt{GNI}}(x_i)\phi_i(x)\) and choose \(v_N(x) = \phi_i(x)\) for any \(i = 1, \ldots, N-1\).  

The GNI discretization of the weak problem reads:
\[
    \txt{look for } u^{\txt{GNI}} = \left[u_N^{\txt{GNI}}(x_j)\right]_{j=0}^N : \begin{cases}
        u_N^{\txt{GNI}}(x_0) = u_N^{\txt{GNI}}(x_N) \\
        \sum_{j=0}^{N} a_N(\phi_j, \phi_i)u_N^{\txt{GNI}} (x_j) = (f, \phi_i)_N & \forall \; i = 1, \ldots, N-1
    \end{cases} 
\]

Now let's have a closer look to the \(\left\{\phi_j\right\}\):
\[
    \phi_j \in \mathbb{P}_N : \phi_j(x_i) = \delta_{ij} = \begin{cases}
        1 & \txt{if } i = j \\
        0 & \txt{otherwise}
    \end{cases}   
\]
Given the discrete inner product \((u,v)_N = \sum_{j=0}^N u(x_j)v(x_j)w_j\) we can write:
\begin{align*}
    (\phi_k, \phi_m)_N &= \sum_{j=0}^{N} \underbrace{\phi_k(x_j)}_{\delta_{kj}} \underbrace{\phi_m(x_j)}_{\delta_{mj}} w_j \quad 0 \leq k,m \leq N \\
    &= \sum_{k=0}^{N} = \begin{cases}
        w_m & \txt{if } k = m \\
        0 & \txt{otherwise}
    \end{cases}
\end{align*}
so \(\left\{\phi_k\right\}\) is orthogonal under the discrete inner product.

The GNI solution is 
\[
    u_N(x) = \sum_{i=0}^{N} \alpha_i\phi_i(x) \quad \left\{\alpha_i\right\} \txt{ unknown coefficients}
\]
Set now \(x = x_j\) with LGL nodes:
\[
    u_N(x_j) = \sum_{i=0}^{N} \alpha_i \underbrace{\phi_i(x_j)}_{\delta{ij}} = \alpha_j
\]
So, given \(u_n^{\txt{GNI}}(x_j)\) the nodal values, we obtain the nodal expansion:
\[
    u_N^{\txt{GNI}}(x) = \sum_{j=0}^{N} u_N^{\txt{GNI}} (x_j)\phi_j(x)
\]
\subsection*{Algebraic form of Spectral GNI}
Now it's about solving the following linear system 
\[
    A^{\txt{GNI}}\vect{u}^{\txt{GNI}} = \vect{f}^{\txt{GNI}}
\]
with \(A_{ij}^{\txt{GNI}} = a_N(\phi_j, \phi_i)\) for \(i = 1,\ldots, N-1, j = 0, \ldots, N\) and \(\vect{f}^{\txt{GNI}} = (f,\phi_i)_N\) for \(i = 1, \ldots, N-1\):
\[
    A^{\txt{GNI}} = \left[\begin{matrix*}
        1 & 0 & \cdots & 0 & 0 \\
        \vdots & \ddots & & &\vdots\\
        \vdots & & a_N(\phi_j, \phi_i) & & \vdots \\
        \vdots & & & \ddots & \vdots \\
        0 & 0 & \cdots & 0 & 1
    \end{matrix*}\right], \quad \vect{f}^{\txt{GNI}} = \left[\begin{matrix*}
        0 \\ \vdots \\ f_i^{\txt{GNI}} \\ \vdots \\ 0
    \end{matrix*}\right]
\]
Given that \(a(u,v) = \int_{-1}^1 \mu u' v' - \int_{-1}^1 bu v' + \int_{-1}^1 \sigma u v\) and \((f,v) = \int_{-1}^1 fv\). We estabilished that \(a_n(u,v) = (\mu u', v')_N - (bu,v')_N + (\sigma u, v)_N\) and that \((f,v)_N = (f,v)_N\), so we obtain 
\[
    A_{ij}^{\txt{GNI}} = a_N(\phi_j, \phi_i) = \underbrace{\left(\mu \phi_j', \phi_i'\right)_N}_{\txt{A}} - \underbrace{\left(b\phi_j, \phi_i'\right)_N}_{\txt{B}}+\underbrace{\left(\sigma \phi_j, \phi_i\right)_N}_{\txt{C}}
\]
Assuming \(\mu, b, \sigma \in \real\) we have that 
\begin{align*}
    C&: \sigma(\phi_j, \phi_i)_N = \sigma \delta_{ij} w_i = \begin{cases}
        \sigma w_i & i = j \\
        0 & i\neq j
    \end{cases} &\rightarrow &\quad M = \sigma \underbrace{\left[\begin{matrix}
        w_0 & 0 & 0 \\ 
        0 & \ddots & 0 \\
        0 & 0 & w_N
    \end{matrix}\right]}_{\txt{diagonal weight matrix}} \\
    B&: - b(\phi_j, \phi_i')_N = -b\sum_{k=0}^{N} \underbrace{\phi_j(x_k)}_{\delta_{jk}}\underbrace{\phi_i'(x_k)}_{D_{ki}\neq 0} w_k &\rightarrow& \quad \txt{full matrix} \\ 
    A&: \mu(\phi_j', \phi_i')_N = \mu\sum_{k=0}^{N} \underbrace{\phi_j'(x_k)}_{D_{kj}}\underbrace{\phi_i'(x_k)}_{D_{ki}} w_k &\rightarrow& \quad \txt{full matrix}
\end{align*}
where \(D = (D_{ki}) = \phi_k'(x_i)\) is the differentiation matrix that can be computed only once. The computation of \((f,\phi_i)_N\) can be made this way 
\[
    (f, \phi_i)_N = \sum_m w_m f(x_m)\underbrace{\phi_i(x_m)}_{\delta_{im}} = w_i f(x_i) 
\]
In conclusion the GNI method is still as full as the spectral one, but much easier to compute thanks to the nodal expansion.
\subsection*{Accuracy}
We can define the Global Lagrange polynomial of degree \(N\) that interpolates \(u\) at LGL nodes as:
\[
    I_nu(x) = \sum_{j=0}^{N}u(x_j)\phi_j(x)
\]
And the interpolation error, for any \(u \in H^{s+1}(-1, 1)\) with \(s \geq 0 \), the interpolation error \(u - I_N u\) satisfies the estimate:
\[
    \norm{u-I_Nu}_{H^k(-1,1)} \leq C(s)\left(\frac{1}{N}\right)^{s+1-k}\norm{u}_{H^{s+1}(-1,1)} \quad \txt{for }k = 0,1
\]
One important feature of LGL nodes is that they are not uniformly spaced (otherwise there could be problems), so that 
\[
    I_nu(x_k) = u(x_k) \quad 0 \leq k \leq N
\]
It's also possible to estimate the \(L^2\) norm of the error as:
\[
    \norm*{u - I_N u}_{L^2(-1,1)} \leq C(s) \left(\frac{1}{N}\right)^{s+1} \norm{u}_{H^{s+1}(-1,1)} \quad s \geq 1
\]
\begin{theorem}[Quadrature error]
    \(\exists \; c > 0 : \forall \; f \in H^q(-1,1)\), with \(q \geq 1\), \(\forall \; v_N \in \mathbb{P}_N\) it holds 
    \[
        \abs*{\int_{-1}^1 f v_N \, dx - (f, v_N)_N} \geq c \left(\frac{1}{N}\right)^q \norm{f}_{H^q(-1,1)} \norm{v_N}_{L^2(-1,1)}
    \]
\end{theorem}
Let now \(u_N^{\txt{GNI}} \in V_N\) be the solution of
\[
     a_N(u_N^{\txt{GNI}}, v_N) = (f, v_N)_N \quad \forall \; v_N \in V_N
\]
If \(u \in H^{s+1}(\Omega)\) and \(f \in H^{s}(\Omega)\) with \(s \geq 0\), then:
\[
    \norm{u-u_N^{\txt{GNI}}}_{H^1(\Omega)} \leq C(s) \left(\frac{1}{N}\right)^s \left(\norm*{u}_{H^{s+1}(\Omega)}+\norm*{f}_{H^s(\Omega)}\right)
\]
So \(u_N^{\txt{GNI}}\) converges with spectral accuracy w.r.t. to \(N\) to the exact solution when the latter is smooth.

\subsubsection*{General ideas}
The idea proposed until now are the following:
\[
    \begin{array}{lccc}
        \txt{(WP)} & V \txt{ Hilbert} & a \txt{ bilinear form} & F \txt{ functional} \\ 
        \txt{(SG)} & V_h \txt{ instead of } V & \txt{same }a &  \txt{same } F \\ 
        \txt{(GNI)} & V_N & a_N  & F_N\\ 
    \end{array}
\]
\begin{itemize}
    \item For the Galerkin method one can use CeÃ  Lemma 
    \begin{align*}
        \honenorm{u-u_N} &\leq \underbrace{\inf_{v_N \in V_N}\honenorm{u-v_N}}_{\txt{distance of } V \txt{ from }V_N} \\
        &\leq \honenorm{u-I_nu}
    \end{align*}
    \item For the Galerkin with Numerical Integration we need something more:
    \begin{align*}
        \honenorm{u-u_N} \leq\quad & \txt{``distance'' of } V \txt{ from }V_N \\
        &+ \txt{``distance'' of } a(\cdot,\cdot) \txt{ from }a_N(\cdot,\cdot) \\
        &+ \txt{``distance'' of } F(\cdot) \txt{ from }F_N(\cdot)
    \end{align*}
\end{itemize}
\subsection{Strang Lemma}
\begin{lemma}[Strang lemma]
Consider the problem
\begin{equation}
    \txt{find } u \in V: a(u,v) = F(v) \quad \forall \; v \in V
    \label{another_weak_formulation}
\end{equation}
and its approximation 
\begin{equation}
    \txt{find } u_h \in V_h: a_h(u_h,v_h) = F_h(v_h) \quad \forall \; v_h \in V_h
    \label{another_weak_approximation}
\end{equation}
with \(\left\{V_h\right\}\) being a family of subspaces of \(V\). Suppose that \(a_h(\cdot,\cdot)\) is continuous on \(V_h \times V_h\) and uniformly coercive on \(V_h\) meaning that:
\[
    \exists \; \alpha^* > 0 \txt{ independent of }h : a_h(v_h, v_h) \geq \alpha^* \norm*{v_h}^2_V \quad \forall \; v_h \in V_h
\]
Also suppose that \(F_h\) is linear and bounded on \(V_h\). Then:
\begin{itemize}
    \item exist a unique solution \(u_h\) to the problem.
    \item such solution depends continuously on the data, i.e. we have 
    \[
        \norm{u_h}_V \leq \frac{1}{\alpha^*} \sup_{v_h \in V_h \backslash \left\{0\right\}} \frac{F_h(v_h)}{\norm*{v_h}_V}
    \]
    \item  finally, the following a priori error estimate holds 
    \begin{align*}
        \norm*{u-u_h}_V \quad \leq & \inf_{w_h \in V_h} \bigg\{\left(1+\frac{M}{\alpha^*}\right) \norm*{u-w_h}_V \\
        &+ \frac{1}{\alpha^*} \sup_{v_h \in V_h \backslash \left\{0\right\}} \frac{\abs*{a(w_h, v_h) - a_h(w_h, v_h)}}{\norm*{v_h}_V} \bigg\} \\
        &+ \frac{1}{\alpha^*} \sup_{v_h \in V_h \backslash \left\{0\right\}} \frac{\abs*{F(v_h)-F_h(v_h)}}{\norm*{v_h}_V} 
    \end{align*}
\end{itemize}
with \(M\) being the continuity constant of \(a(\cdot, \cdot)\)
\end{lemma}
\begin{proof}
    The assumption of Lax-Milgram are satisfied for \eqref{another_weak_approximation}, so the solution exists and is unique. Moreover
    \[
        \norm*{u_h}_V \leq \frac{1}{\alpha^*} \norm*{F_h}_{V'_h} 
    \]
    with \(\norm{F_h}_{V'_h} = \sup_{v_h \in V_h \backslash \left\{0\right\}} \frac{F_h(v_h)}{\norm{v_h}_V}\) being the norm of the dual space \(V'_h\).

    Now the only thing missing is the error inequality. Let \(w_h\) be any function of the subspace \(V_h\). Setting \(\sigma_h = u_h -w_h \in V_h\), we have:
    \begin{align*}
        \alpha^* \norm*{\sigma_h}^2_V &\leq a_h(\sigma_h, \sigma_h) \tag*{(by coercivity of \(a_h\))} \\
        &= a_h(u_h, \sigma_h) - a_h(w_h, \sigma_h) \\
        &= F_h(\sigma_h) - a_h(w_h, \sigma_h) \tag*{(by \eqref{another_weak_approximation})} \\
        &= F_h(\sigma_h) - F(\sigma_h) + F(\sigma_h) - a_h(w_h, \sigma_h) \\
        &= \left[F_h(\sigma_h)-F(\sigma_h)\right] + a(u, \sigma_h) - a_h(w_h, \sigma_h) \tag*{(by \eqref{another_weak_formulation})} \\
        &= \left[F_h(\sigma_h) - F(\sigma_h)\right] + a(u-w_h, \sigma_h) + \left[a(w_h, \sigma_h) - a_h(w_h, \sigma_h)\right]
    \end{align*}
    If \(\sigma_h \neq 0\)
\end{proof}