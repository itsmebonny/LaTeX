@article{allardSOFAOpenSource,
  title = {{{SOFA}} - an {{Open Source Framework}} for {{Medical Simulation}}},
  author = {Allard, J{\'e}r{\'e}mie and Cotin, St{\'e}phane and Faure, Fran{\c c}ois and Bensoussan, Pierre-Jean and Poyer, Fran{\c c}ois and Duriez, Christian and Delingette, Herv{\'e} and Grisoni, Laurent},
  abstract = {SOFA is a new open source framework primarily targeted at medical simulation research. Based on an advanced software architecture, it allows to (1) create complex and evolving simulations by combining new algorithms with algorithms already included in SOFA; (2) modify most parameters of the simulation -- deformable behavior, surface representation, solver, constraints, collision algorithm, etc. -- by simply editing an XML file; (3) build complex models from simpler ones using a scene-graph description; (4) efficiently simulate the dynamics of interacting objects using abstract equation solvers; and (5) reuse and easily compare a variety of available methods. In this paper we highlight the key concepts of the SOFA architecture and illustrate its potential through a series of examples.},
  langid = {english},
  file = {/home/andrea/Zotero/storage/9EL5KYAK/Allard et al. - SOFA - an Open Source Framework for Medical Simula.pdf}
}

@misc{raissi2024physicsinformedneuralnetworksextensions,
      title={Physics-Informed Neural Networks and Extensions}, 
      author={Maziar Raissi and Paris Perdikaris and Nazanin Ahmadi and George Em Karniadakis},
      year={2024},
      eprint={2408.16806},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2408.16806}, 
}

@article{chenGraphNeuralNetworks2021,
  title = {Graph Neural Networks for Laminar Flow Prediction around Random {{2D}} Shapes},
  author = {Chen, Junfeng and Hachem, Elie and Viquerat, Jonathan},
  year = {2021},
  month = dec,
  journal = {Physics of Fluids},
  volume = {33},
  number = {12},
  eprint = {2107.11529},
  primaryclass = {physics},
  pages = {123607},
  issn = {1070-6631, 1089-7666},
  doi = {10.1063/5.0064108},
  urldate = {2024-03-21},
  abstract = {In the recent years, the domain of fast flow field prediction has been vastly dominated by pixel-based convolutional neural networks. Yet, the recent advent of graph convolutional neural networks (GCNNs) have attracted a considerable attention in the computational fluid dynamics (CFD) community. In this contribution, we proposed a GCNN structure as a surrogate model for laminar flow prediction around 2D obstacles. Unlike traditional convolution on image pixels, the graph convolution can be directly applied on body-fitted triangular meshes, hence yielding an easy coupling with CFD solvers. The proposed GCNN model is trained over a data set composed of CFD-computed laminar flows around 2,000 random 2D shapes. Accuracy levels are assessed on reconstructed velocity and pressure fields around out-of-training obstacles, and are compared with that of standard U-net architectures, especially in the boundary layer area.},
  archiveprefix = {arXiv},
  keywords = {graphs,Physics - Computational Physics,Physics - Fluid Dynamics},
  file = {/home/andrea/Zotero/storage/RUQDSVST/Chen et al. - 2021 - Graph neural networks for laminar flow prediction .pdf;/home/andrea/Zotero/storage/589H89LP/2107.html}
}

@misc{paszke2019pytorchimperativestylehighperformance,
      title={PyTorch: An Imperative Style, High-Performance Deep Learning Library}, 
      author={Adam Paszke and Sam Gross and Francisco Massa and Adam Lerer and James Bradbury and Gregory Chanan and Trevor Killeen and Zeming Lin and Natalia Gimelshein and Luca Antiga and Alban Desmaison and Andreas KÃ¶pf and Edward Yang and Zach DeVito and Martin Raison and Alykhan Tejani and Sasank Chilamkurthy and Benoit Steiner and Lu Fang and Junjie Bai and Soumith Chintala},
      year={2019},
      eprint={1912.01703},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1912.01703}, 
}

@article{Liu_1989,
  abstract = {We study the numerical performance of a limited memory quasi-Newton method for large scale optimization, which we call the L-BFGS method. We compare its performance with that of the method developed by Buckley and LeNir (1985), which combines cycles of BFGS steps and conjugate direction steps. Our numerical tests indicate that the L-BFGS method is faster than the method of Buckleyand LeNir, and is better able to use additional storage to accelerate convergence. We show that the L-BFGS method can be greatly accelerated by means of a simple scaling. We then compare the L-BFGS method with the partitioned quasi-Newton method of Griewank and Toint (1982a). The results show that, for some problems, the partitioned quasi-Newton method is clearly superior to the L-BFGS method. However we find that for other problems the L-BFGS method is very competitive due to its low iteration cost. We also study the convergence properties of the L-BFGS method, and prove global convergence On uniformly convex problems.},
  added-at = {2010-05-10T08:12:01.000+0200},
  author = {Liu, Dong C. and Nocedal, Jorge},
  biburl = {https://www.bibsonomy.org/bibtex/20831baca30a7a5c6b4c07f8b258bd5e3/dhruvbansal},
  file = {/home/dhruv/projects/work/papers/papers/Liu_1989.pdf},
  interhash = {3c07caded0ed60d42c5dc813b5459b83},
  intrahash = {0831baca30a7a5c6b4c07f8b258bd5e3},
  keywords = {algorithms computational_methods, optimization,},
  pages = {503-528},
  read = {nil},
  timestamp = {2010-05-10T08:12:05.000+0200},
  title = {On the limited memory BFGS method for large scale optimization},
  volume = 45,
  year = 1989
}



@misc{tensorflow2015-whitepaper,
title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url={https://www.tensorflow.org/},
note={Software available from tensorflow.org},
author={
    Mart\'{i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dandelion~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
  year={2015},
}

@misc{djeumouNeuralNetworksPhysicsInformed2022,
  title = {Neural {{Networks}} with {{Physics-Informed Architectures}} and {{Constraints}} for {{Dynamical Systems Modeling}}},
  author = {Djeumou, Franck and Neary, Cyrus and Goubault, Eric and Putot, Sylvie and Topcu, Ufuk},
  year = {2022},
  month = dec,
  number = {arXiv:2109.06407},
  eprint = {2109.06407},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2109.06407},
  urldate = {2024-01-17},
  abstract = {Effective inclusion of physics-based knowledge into deep neural network models of dynamical systems can greatly improve data efficiency and generalization. Such a-priori knowledge might arise from physical principles (e.g., conservation laws) or from the system's design (e.g., the Jacobian matrix of a robot), even if large portions of the system dynamics remain unknown. We develop a framework to learn dynamics models from trajectory data while incorporating a-priori system knowledge as inductive bias. More specifically, the proposed framework uses physics-based side information to inform the structure of the neural network itself, and to place constraints on the values of the outputs and the internal states of the model. It represents the system's vector field as a composition of known and unknown functions, the latter of which are parametrized by neural networks. The physics-informed constraints are enforced via the augmented Lagrangian method during the model's training. We experimentally demonstrate the benefits of the proposed approach on a variety of dynamical systems -- including a benchmark suite of robotics environments featuring large state spaces, non-linear dynamics, external forces, contact forces, and control inputs. By exploiting a-priori system knowledge during training, the proposed approach learns to predict the system dynamics two orders of magnitude more accurately than a baseline approach that does not include prior knowledge, given the same training dataset.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/andrea/Zotero/storage/HNNEF5BI/Djeumou et al. - 2022 - Neural Networks with Physics-Informed Architecture.pdf;/home/andrea/Zotero/storage/WBNHQWQR/2109.html}
}

@article{fathiSuperresolutionDenoising4DFlow2020,
  title = {Super-Resolution and Denoising of {{4D-Flow MRI}} Using Physics-{{Informed}} Deep Neural Nets},
  author = {Fathi, Mojtaba F. and {Perez-Raya}, Isaac and Baghaie, Ahmadreza and Berg, Philipp and Janiga, Gabor and Arzani, Amirhossein and D'Souza, Roshan M.},
  year = {2020},
  month = dec,
  journal = {Computer Methods and Programs in Biomedicine},
  volume = {197},
  pages = {105729},
  issn = {0169-2607},
  doi = {10.1016/j.cmpb.2020.105729},
  urldate = {2024-03-18},
  abstract = {Background and Objective: Time resolved three-dimensional phase contrast magnetic resonance imaging (4D-Flow MRI) has been used to non-invasively measure blood velocities in the human vascular system. However, issues such as low spatio-temporal resolution, acquisition noise, velocity aliasing, and phase-offset artifacts have hampered its clinical application. In this research, we developed a purely data-driven method for super-resolution and denoising of 4D-Flow MRI. Methods: The flow velocities, pressure, and the MRI image magnitude are modeled as a patient-specific deep neural net (DNN). For training, 4D-Flow MRI images in the complex Cartesian space are used to impose data-fidelity. Physics of fluid flow is imposed through regularization. Creative loss function terms have been introduced to handle noise and super-resolution. The trained patient-specific DNN can be sampled to generate noise-free high-resolution flow images. The proposed method has been implemented using the TensorFlow DNN library and tested on numerical phantoms and validated in-vitro using high-resolution particle image velocitmetry (PIV) and 4D-Flow MRI experiments on transparent models subjected to pulsatile flow conditions. Results: In case of numerical phantoms, we were able to increase spatial resolution by a factor of 100 and temporal resolution by a factor of 5 compared to the simulated 4D-Flow MRI. There is an order of magnitude reduction of velocity normalized root mean square error (vNRMSE). In case of the in-vitro validation tests with PIV as reference, there is similar improvement in spatio-temporal resolution. Although the vNRMSE is reduced by 50\%, the method is unable to negate a systematic bias with respect to the reference PIV that is introduced by the 4D-Flow MRI measurement. Conclusions: This work has demonstrated the feasibility of using the readily available machinery of deep learning to enhance 4D-Flow MRI using a purely data-driven method. Unlike current state-of-the-art methods, the proposed method is agnostic to geometry and boundary conditions and therefore eliminates the need for tedious tasks such as accurate image segmentation for geometry, image registration, and estimation of boundary flow conditions. Arbitrary regions of interest can be selected for processing. This work will lead to user-friendly analysis tools that will enable quantitative hemodynamic analysis of vascular diseases in a clinical setting.},
  keywords = {4D-Flow MRI,Data assimilation,Denoising,PI-DNN,super-resolution,Super-resolution,Validation},
  file = {/home/andrea/Zotero/storage/E3L9DMCM/Fathi et al. - 2020 - Super-resolution and denoising of 4D-Flow MRI usin.pdf;/home/andrea/Zotero/storage/F3P7JUXK/S0169260720315625.html}
}

@misc{fortunatoMultiScaleMeshGraphNets2022,
  title = {{{MultiScale MeshGraphNets}}},
  author = {Fortunato, Meire and Pfaff, Tobias and Wirnsberger, Peter and Pritzel, Alexander and Battaglia, Peter},
  year = {2022},
  month = oct,
  number = {arXiv:2210.00612},
  eprint = {2210.00612},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2210.00612},
  urldate = {2024-03-18},
  abstract = {In recent years, there has been a growing interest in using machine learning to overcome the high cost of numerical simulation, with some learned models achieving impressive speed-ups over classical solvers whilst maintaining accuracy. However, these methods are usually tested at low-resolution settings, and it remains to be seen whether they can scale to the costly high-resolution simulations that we ultimately want to tackle. In this work, we propose two complementary approaches to improve the framework from MeshGraphNets, which demonstrated accurate predictions in a broad range of physical systems. MeshGraphNets relies on a message passing graph neural network to propagate information, and this structure becomes a limiting factor for high-resolution simulations, as equally distant points in space become further apart in graph space. First, we demonstrate that it is possible to learn accurate surrogate dynamics of a high-resolution system on a much coarser mesh, both removing the message passing bottleneck and improving performance; and second, we introduce a hierarchical approach (MultiScale MeshGraphNets) which passes messages on two different resolutions (fine and coarse), significantly improving the accuracy of MeshGraphNets while requiring less computational resources.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computational Engineering Finance and Science,Computer Science - Machine Learning,crazy methods},
  file = {/home/andrea/Zotero/storage/LW94A782/Fortunato et al. - 2022 - MultiScale MeshGraphNets.pdf;/home/andrea/Zotero/storage/LHNJQWGZ/2210.html}
}

@article{fukamiSuperResolutionAnalysisMachine2023,
  title = {Super-{{Resolution Analysis}} via {{Machine Learning}}: {{A Survey}} for {{Fluid Flows}}},
  shorttitle = {Super-{{Resolution Analysis}} via {{Machine Learning}}},
  author = {Fukami, Kai and Fukagata, Koji and Taira, Kunihiko},
  year = {2023},
  month = aug,
  journal = {Theoretical and Computational Fluid Dynamics},
  volume = {37},
  number = {4},
  eprint = {2301.10937},
  primaryclass = {physics},
  pages = {421--444},
  issn = {0935-4964, 1432-2250},
  doi = {10.1007/s00162-023-00663-0},
  urldate = {2024-03-15},
  abstract = {This paper surveys machine-learning-based super-resolution reconstruction for vortical flows. Super resolution aims to find the high-resolution flow fields from low-resolution data and is generally an approach used in image reconstruction. In addition to surveying a variety of recent super-resolution applications, we provide case studies of super-resolution analysis for an example of two-dimensional decaying isotropic turbulence. We demonstrate that physics-inspired model designs enable successful reconstruction of vortical flows from spatially limited measurements. We also discuss the challenges and outlooks of machine-learning-based super-resolution analysis for fluid flow applications. The insights gained from this study can be leveraged for super-resolution analysis of numerical and experimental flow data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Physics - Computational Physics,Physics - Fluid Dynamics,super-resolution},
  file = {/home/andrea/Zotero/storage/S4D7DE4C/Fukami et al. - 2023 - Super-Resolution Analysis via Machine Learning A .pdf;/home/andrea/Zotero/storage/MNP4RX6G/2301.html}
}

@article{gaoPhysicsinformedGraphNeural2022,
  title = {Physics-Informed Graph Neural {{Galerkin}} Networks: {{A}} Unified Framework for Solving {{PDE-governed}} Forward and Inverse Problems},
  shorttitle = {Physics-Informed Graph Neural {{Galerkin}} Networks},
  author = {Gao, Han and Zahr, Matthew J. and Wang, Jian-Xun},
  year = {2022},
  month = feb,
  journal = {Computer Methods in Applied Mechanics and Engineering},
  volume = {390},
  eprint = {2107.12146},
  primaryclass = {cs},
  pages = {114502},
  issn = {00457825},
  doi = {10.1016/j.cma.2021.114502},
  urldate = {2024-03-23},
  abstract = {Despite the great promise of the physics-informed neural networks (PINNs) in solving forward and inverse problems, several technical challenges are present as roadblocks for more complex and realistic applications. First, most existing PINNs are based on point-wise formulation with fully-connected networks to learn continuous functions, which suffer from poor scalability and hard boundary enforcement. Second, the infinite search space over-complicates the non-convex optimization for network training. Third, although the convolutional neural network (CNN)-based discrete learning can significantly improve training efficiency, CNNs struggle to handle irregular geometries with unstructured meshes. To properly address these challenges, we present a novel discrete PINN framework based on graph convolutional network (GCN) and variational structure of PDE to solve forward and inverse partial differential equations (PDEs) in a unified manner. The use of a piecewise polynomial basis can reduce the dimension of search space and facilitate training and convergence. Without the need of tuning penalty parameters in classic PINNs, the proposed method can strictly impose boundary conditions and assimilate sparse data in both forward and inverse settings. The flexibility of GCNs is leveraged for irregular geometries with unstructured meshes. The effectiveness and merit of the proposed method are demonstrated over a variety of forward and inverse computational mechanics problems governed by both linear and nonlinear PDEs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computational Engineering Finance and Science},
  file = {/home/andrea/Zotero/storage/ART8HICB/Gao et al. - 2022 - Physics-informed graph neural Galerkin networks A.pdf;/home/andrea/Zotero/storage/6Q2T3HEC/2107.html}
}

@article{gaoSuperresolutionDenoisingFluid2021,
  title = {Super-Resolution and Denoising of Fluid Flow Using Physics-Informed Convolutional Neural Networks without High-Resolution Labels},
  author = {Gao, Han and Sun, Luning and Wang, Jian-Xun},
  year = {2021},
  month = jul,
  journal = {Physics of Fluids},
  volume = {33},
  number = {7},
  eprint = {2011.02364},
  primaryclass = {physics},
  pages = {073603},
  issn = {1070-6631, 1089-7666},
  doi = {10.1063/5.0054312},
  urldate = {2024-03-15},
  abstract = {High-resolution (HR) information of fluid flows, although preferable, is usually less accessible due to limited computational or experimental resources. In many cases, fluid data are generally sparse, incomplete, and possibly noisy. How to enhance spatial resolution and decrease the noise level of flow data is essential and practically useful. Deep learning (DL) techniques have been demonstrated to be effective for super-resolution (SR) tasks, which, however, primarily rely on sufficient HR labels for training. In this work, we present a novel physics-informed DL-based SR solution using convolutional neural networks (CNN), which is able to produce HR flow fields from low-resolution (LR) inputs in high-dimensional parameter space. By leveraging the conservation laws and boundary conditions of fluid flows, the CNN-SR model is trained without any HR labels. Moreover, the proposed CNN-SR solution unifies the forward SR and inverse data assimilation for the scenarios where the physics is partially known, e.g., unknown boundary conditions. Several flow SR problems relevant to cardiovascular applications have been studied to demonstrate the proposed method's effectiveness and merit. Both Gaussian and non-Gaussian MRI noises are investigated to illustrate the denoising capability.},
  archiveprefix = {arXiv},
  keywords = {Physics - Computational Physics,Physics - Fluid Dynamics,super-resolution},
  file = {/home/andrea/Zotero/storage/8RY7CYK7/Gao et al. - 2021 - Super-resolution and denoising of fluid flow using.pdf;/home/andrea/Zotero/storage/AISHHC7A/2011.html}
}

@misc{hanPredictingPhysicsMeshreduced2022a,
  title = {Predicting {{Physics}} in {{Mesh-reduced Space}} with {{Temporal Attention}}},
  author = {Han, Xu and Gao, Han and Pfaff, Tobias and Wang, Jian-Xun and Liu, Li-Ping},
  year = {2022},
  month = may,
  number = {arXiv:2201.09113},
  eprint = {2201.09113},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2201.09113},
  urldate = {2024-03-20},
  abstract = {Graph-based next-step prediction models have recently been very successful in modeling complex high-dimensional physical systems on irregular meshes. However, due to their short temporal attention span, these models suffer from error accumulation and drift. In this paper, we propose a new method that captures long-term dependencies through a transformer-style temporal attention model. We introduce an encoder-decoder structure to summarize features and create a compact mesh representation of the system state, to allow the temporal model to operate on a low-dimensional mesh representations in a memory efficient manner. Our method outperforms a competitive GNN baseline on several complex fluid dynamics prediction tasks, from sonic shocks to vascular flow. We demonstrate stable rollouts without the need for training noise and show perfectly phase-stable predictions even for very long sequences. More broadly, we believe our approach paves the way to bringing the benefits of attention-based sequence models to solving high-dimensional complex physics tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,crazy methods},
  file = {/home/andrea/Zotero/storage/KG6T88B2/Han et al. - 2022 - Predicting Physics in Mesh-reduced Space with Temp.pdf;/home/andrea/Zotero/storage/6RFEJ35U/2201.html}
}

@misc{HttpsInriaHal,
  title = {{{https://inria.hal.science/inria-00319416/document}}},
  urldate = {2024-07-09},
  howpublished = {https://inria.hal.science/inria-00319416/document}
}

@misc{jiangMeshfreeFlowNetPhysicsConstrainedDeep2020,
  title = {{{MeshfreeFlowNet}}: {{A Physics-Constrained Deep Continuous Space-Time Super-Resolution Framework}}},
  shorttitle = {{{MeshfreeFlowNet}}},
  author = {Jiang, Chiyu Max and Esmaeilzadeh, Soheil and Azizzadenesheli, Kamyar and Kashinath, Karthik and Mustafa, Mustafa and Tchelepi, Hamdi A. and Marcus, Philip and Prabhat and Anandkumar, Anima},
  year = {2020},
  month = aug,
  number = {arXiv:2005.01463},
  eprint = {2005.01463},
  primaryclass = {physics, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2005.01463},
  urldate = {2024-03-23},
  abstract = {We propose MeshfreeFlowNet, a novel deep learning-based super-resolution framework to generate continuous (grid-free) spatio-temporal solutions from the low-resolution inputs. While being computationally efficient, MeshfreeFlowNet accurately recovers the fine-scale quantities of interest. MeshfreeFlowNet allows for: (i) the output to be sampled at all spatio-temporal resolutions, (ii) a set of Partial Differential Equation (PDE) constraints to be imposed, and (iii) training on fixed-size inputs on arbitrarily sized spatio-temporal domains owing to its fully convolutional encoder. We empirically study the performance of MeshfreeFlowNet on the task of super-resolution of turbulent flows in the Rayleigh-Benard convection problem. Across a diverse set of evaluation metrics, we show that MeshfreeFlowNet significantly outperforms existing baselines. Furthermore, we provide a large scale implementation of MeshfreeFlowNet and show that it efficiently scales across large clusters, achieving 96.80\% scaling efficiency on up to 128 GPUs and a training time of less than 4 minutes.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing,Physics - Fluid Dynamics,Statistics - Machine Learning},
  file = {/home/andrea/Zotero/storage/YN77965Z/Jiang et al. - 2020 - MeshfreeFlowNet A Physics-Constrained Deep Contin.pdf;/home/andrea/Zotero/storage/8QT4MPCZ/2005.html}
}

@misc{kagPhysicsinformedNeuralNetwork2024,
  title = {Physics-Informed Neural Network for Modeling Dynamic Linear Elasticity},
  author = {Kag, Vijay and Gopinath, Venkatesh},
  year = {2024},
  month = jan,
  number = {arXiv:2312.15175},
  eprint = {2312.15175},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.15175},
  urldate = {2024-04-29},
  abstract = {In this work, we present the physics-informed neural network (PINN) model applied particularly to dynamic problems in solid mechanics. We focus on forward and inverse problems. Particularly, we show how a PINN model can be used efficiently for material identification in a dynamic setting. In this work, we assume linear continuum elasticity. We show results for two-dimensional (2D) plane strain problem and then we proceed to apply the same techniques for a three-dimensional (3D) problem. As for the training data we use the solution based on the finite element method. We rigorously show that PINN models are accurate, robust and computationally efficient, especially as a surrogate model for material identification problems. Also, we employ state-of-the-art techniques from the PINN literature which are an improvement to the vanilla implementation of PINN. Based on our results, we believe that the framework we have developed can be readily adapted to computational platforms for solving multiple dynamic problems in solid mechanics.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Neural and Evolutionary Computing},
  file = {/home/andrea/Zotero/storage/SGAYXITF/Kag and Gopinath - 2024 - Physics-informed neural network for modeling dynam.pdf;/home/andrea/Zotero/storage/JPCQVYKJ/2312.html}
}

@article{kienerDatadrivenCorrectionCoarse2023,
  title = {Data-Driven Correction of Coarse Grid {{CFD}} Simulations},
  author = {Kiener, A. and Langer, S. and Bekemeyer, P.},
  year = {2023},
  month = oct,
  journal = {Computers \& Fluids},
  volume = {264},
  pages = {105971},
  issn = {0045-7930},
  doi = {10.1016/j.compfluid.2023.105971},
  urldate = {2024-04-29},
  abstract = {Computational fluid dynamics is a cornerstone of the modern aerospace industry, providing important insights through aerodynamic analysis while reducing the need for expensive experiments and tests. A consistent spatial discretization on so-called grids approximates the analytical solution of the partial differential equation with increasing number of discrete points. But computing a rigorous amount of CFD simulations on fine grids can be a daunting task due to the high computational cost involved. Thus, it is of interest to find methods which generate accurate results while keeping computational costs low. This work proofs that machine learning as a post-processing tool is capable of improving the accuracy of coarse grid simulations. The results show that three machine learning models with varying complexity, namely random forest, neural network, and graph neural network, are capable of finding patterns in coarse grid simulations. These patterns are used for a vertex-wise prediction of the discretization error to approximate the field variables of interest of the corresponding fine grid simulation mapped onto the coarse grid. Initial training and testing is performed on the two dimensional RAE2822 airfoil leading to corrected flow fields, improved surface integrals and coefficients, even when shocks are present. Additional tests are performed on the RAE5212 airfoil, showing the generalization limits of the trained models. Finally, the training and prediction is showcased on a three dimensional geometry. The proposed method promises to reduce computational expenses while increasing the accuracy of the coarse grid results which works locally, e.g. it corrects the error for each vertex individually and is therefore not restricted by the number of grid points. The presented results obtained by the machine learning models during post-processing are a promising baseline for more integrated developments, where the models will interact in a dynamic fashion with the flow solver to further improve coarse grid simulations.},
  keywords = {CFD,Coarse grid,Error correction,Graph neural network},
  file = {/home/andrea/Zotero/storage/U6AILXK9/S0045793023001962.html}
}

@misc{luzLearningAlgebraicMultigrid2020,
  title = {Learning {{Algebraic Multigrid Using Graph Neural Networks}}},
  author = {Luz, Ilay and Galun, Meirav and Maron, Haggai and Basri, Ronen and Yavneh, Irad},
  year = {2020},
  month = sep,
  number = {arXiv:2003.05744},
  eprint = {2003.05744},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2003.05744},
  urldate = {2024-03-25},
  abstract = {Efficient numerical solvers for sparse linear systems are crucial in science and engineering. One of the fastest methods for solving large-scale sparse linear systems is algebraic multigrid (AMG). The main challenge in the construction of AMG algorithms is the selection of the prolongation operator -- a problem-dependent sparse matrix which governs the multiscale hierarchy of the solver and is critical to its efficiency. Over many years, numerous methods have been developed for this task, and yet there is no known single right answer except in very special cases. Here we propose a framework for learning AMG prolongation operators for linear systems with sparse symmetric positive (semi-) definite matrices. We train a single graph neural network to learn a mapping from an entire class of such matrices to prolongation operators, using an efficient unsupervised loss function. Experiments on a broad class of problems demonstrate improved convergence rates compared to classical AMG, demonstrating the potential utility of neural networks for developing sparse system solvers.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/andrea/Zotero/storage/TV7WEXTH/Luz et al. - 2020 - Learning Algebraic Multigrid Using Graph Neural Ne.pdf;/home/andrea/Zotero/storage/N3QUQT39/2003.html}
}

@article{mendizabalSimulationHyperelasticMaterials2020,
  title = {Simulation of Hyperelastic Materials in Real-Time Using {{Deep Learning}}},
  author = {Mendizabal, Andrea and {M{\'a}rquez-Neila}, Pablo and Cotin, St{\'e}phane},
  year = {2020},
  month = jan,
  journal = {Medical Image Analysis},
  volume = {59},
  eprint = {1904.06197},
  primaryclass = {physics},
  pages = {101569},
  issn = {13618415},
  doi = {10.1016/j.media.2019.101569},
  urldate = {2024-01-17},
  abstract = {The finite element method (FEM) is among the most commonly used numerical methods for solving engineering problems. Due to its computational cost, various ideas have been introduced to reduce computation times, such as domain decomposition, parallel computing, adaptive meshing, and model order reduction. In this paper we present U-Mesh: a data-driven method based on a U-Net architecture that approximates the non-linear relation between a contact force and the displacement field computed by a FEM algorithm. We show that deep learning, one of the latest machine learning methods based on artificial neural networks, can enhance computational mechanics through its ability to encode highly non-linear models in a compact form. Our method is applied to two benchmark examples: a cantilever beam and an L-shape subject to moving punctual loads. A comparison between our method and proper orthogonal decomposition (POD) is done through the paper. The results show that U-Mesh can perform very fast simulations on various geometries, mesh resolutions and number of input forces with very small errors.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computational Engineering Finance and Science,Computer Science - Machine Learning,Physics - Computational Physics},
  file = {/home/andrea/Zotero/storage/QJKXMRB6/Mendizabal et al. - 2020 - Simulation of hyperelastic materials in real-time .pdf;/home/andrea/Zotero/storage/4RF89JXD/1904.html}
}

@misc{odotDeepPhysicsPhysicsAware2021,
  title = {{{DeepPhysics}}: A Physics Aware Deep Learning Framework for Real-Time Simulation},
  shorttitle = {{{DeepPhysics}}},
  author = {Odot, Alban and Haferssas, Ryadh and Cotin, St{\'e}phane},
  year = {2021},
  month = sep,
  number = {arXiv:2109.09491},
  eprint = {2109.09491},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2109.09491},
  urldate = {2024-04-29},
  abstract = {Real-time simulation of elastic structures is essential in many applications, from computer-guided surgical interventions to interactive design in mechanical engineering. The Finite Element Method is often used as the numerical method of reference for solving the partial differential equations associated with these problems. Yet, deep learning methods have recently shown that they could represent an alternative strategy to solve physics-based problems 1,2,3. In this paper, we propose a solution to simulate hyper-elastic materials using a data-driven approach, where a neural network is trained to learn the non-linear relationship between boundary conditions and the resulting displacement field. We also introduce a method to guarantee the validity of the solution. In total, we present three contributions: an optimized data set generation algorithm based on modal analysis, a physics-informed loss function, and a Hybrid Newton-Raphson algorithm. The method is applied to two benchmarks: a cantilever beam and a propeller. The results show that our network architecture trained with a limited amount of data can predict the displacement field in less than a millisecond. The predictions on various geometries, topologies, mesh resolutions, and boundary conditions are accurate to a few micrometers for non-linear deformations of several centimeters of amplitude.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/andrea/Zotero/storage/ZLVM49PD/Odot et al. - 2021 - DeepPhysics a physics aware deep learning framewo.pdf;/home/andrea/Zotero/storage/T5X9VL6Q/2109.html}
}

@misc{odotRealtimeElasticPartial2023,
  title = {Real-Time Elastic Partial Shape Matching Using a Neural Network-Based Adjoint Method},
  author = {Odot, Alban and Mestdagh, Guillaume and Privat, Yannick and Cotin, St{\'e}phane},
  year = {2023},
  month = mar,
  number = {arXiv:2303.09343},
  eprint = {2303.09343},
  primaryclass = {cs, math},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.09343},
  urldate = {2024-04-29},
  abstract = {Surface matching usually provides significant deformations that can lead to structural failure due to the lack of physical policy. In this context, partial surface matching of non-linear deformable bodies is crucial in engineering to govern structure deformations. In this article, we propose to formulate the registration problem as an optimal control problem using an artificial neural network where the unknown is the surface force distribution that applies to the object and the resulting deformation computed using a hyper-elastic model. The optimization problem is solved using an adjoint method where the hyper-elastic problem is solved using the feed-forward neural network and the adjoint problem is obtained through the backpropagation of the network. Our process improves the computation speed by multiple orders of magnitude while providing acceptable registration errors.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Mathematics - Optimization and Control},
  file = {/home/andrea/Zotero/storage/F8YGBT36/Odot et al. - 2023 - Real-time elastic partial shape matching using a n.pdf;/home/andrea/Zotero/storage/ICIK22A4/2303.html}
}

@misc{pfaffLearningMeshBasedSimulation2021a,
  title = {Learning {{Mesh-Based Simulation}} with {{Graph Networks}}},
  author = {Pfaff, Tobias and Fortunato, Meire and {Sanchez-Gonzalez}, Alvaro and Battaglia, Peter W.},
  year = {2021},
  month = jun,
  number = {arXiv:2010.03409},
  eprint = {2010.03409},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2010.03409},
  urldate = {2024-01-17},
  abstract = {Mesh-based simulations are central to modeling complex physical systems in many disciplines across science and engineering. Mesh representations support powerful numerical integration methods and their resolution can be adapted to strike favorable trade-offs between accuracy and efficiency. However, high-dimensional scientific simulations are very expensive to run, and solvers and parameters must often be tuned individually to each system studied. Here we introduce MeshGraphNets, a framework for learning mesh-based simulations using graph neural networks. Our model can be trained to pass messages on a mesh graph and to adapt the mesh discretization during forward simulation. Our results show it can accurately predict the dynamics of a wide range of physical systems, including aerodynamics, structural mechanics, and cloth. The model's adaptivity supports learning resolution-independent dynamics and can scale to more complex state spaces at test time. Our method is also highly efficient, running 1-2 orders of magnitude faster than the simulation on which it is trained. Our approach broadens the range of problems on which neural network simulators can operate and promises to improve the efficiency of complex, scientific modeling tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computational Engineering Finance and Science,Computer Science - Machine Learning},
  file = {/home/andrea/Zotero/storage/CG7ZVBT7/Pfaff et al. - 2021 - Learning Mesh-Based Simulation with Graph Networks.pdf;/home/andrea/Zotero/storage/FXUGV7BX/2010.html}
}
@misc{tonello2019machinelearningtipstricks,
      title={Machine Learning Tips and Tricks for Power Line Communications}, 
      author={Andrea M. Tonello and Nunzio A. Letizia and Davide Righini and Francesco Marcuzzi},
      year={2019},
      eprint={1904.11949},
      archivePrefix={arXiv},
      primaryClass={eess.SP},
      url={https://arxiv.org/abs/1904.11949}, 
}

 @article{Wang_Du_Coros_Thomaszewski_2024, title={Neural Modes: Self-supervised Learning of Nonlinear Modal Subspaces}, url={http://arxiv.org/abs/2404.17620}, DOI={10.48550/arXiv.2404.17620}, abstractNote={We propose a self-supervised approach for learning physics-based subspaces for real-time simulation. Existing learning-based methods construct subspaces by approximating pre-defined simulation data in a purely geometric way. However, this approach tends to produce high-energy configurations, leads to entangled latent space dimensions, and generalizes poorly beyond the training set. To overcome these limitations, we propose a self-supervised approach that directly minimizes the systemâs mechanical energy during training. We show that our method leads to learned subspaces that reflect physical equilibrium constraints, resolve overfitting issues of previous methods, and offer interpretable latent space parameters.}, note={arXiv:2404.17620 [cs]}, number={arXiv:2404.17620}, publisher={arXiv}, author={Wang, Jiahong and Du, Yinwei and Coros, Stelian and Thomaszewski, Bernhard}, year={2024}, month=apr }


 @article{Pentland_Williams_1989, title={Good vibrations: modal dynamics for graphics and animation}, volume={23}, ISSN={0097-8930}, DOI={10.1145/74334.74355}, number={3}, journal={SIGGRAPH Comput. Graph.}, author={Pentland, A. and Williams, J.}, year={1989}, month=jul, pages={207â214} }


 @article{Tierz_Alfaro_GonzÃ¡lez_Chinesta_Cueto_2025, title={Graph neural networks informed locally by thermodynamics}, url={http://arxiv.org/abs/2405.13093}, DOI={10.48550/arXiv.2405.13093}, abstractNote={Thermodynamics-informed neural networks employ inductive biases for the enforcement of the first and second principles of thermodynamics. To construct these biases, a metriplectic evolution of the system is assumed. This provides excellent results, when compared to uninformed, black box networks. While the degree of accuracy can be increased in one or two orders of magnitude, in the case of graph networks, this requires assembling global Poisson and dissipation matrices, which breaks the local structure of such networks. In order to avoid this drawback, a local version of the metriplectic biases has been developed in this work, which avoids the aforementioned matrix assembly, thus preserving the node-by-node structure of the graph networks. We apply this framework for examples in the fields of solid and fluid mechanics. Our approach demonstrates significant computational efficiency and strong generalization capabilities, accurately making inferences on examples significantly different from those encountered during training.}, note={arXiv:2405.13093 [cs]}, number={arXiv:2405.13093}, publisher={arXiv}, author={Tierz, Alicia and Alfaro, Iciar and GonzÃ¡lez, David and Chinesta, Francisco and Cueto, ElÃ­as}, year={2025}, month=jan }



 @article{Duenser_Thomaszewski_Poranne_Coros_2022, address={New York, NY, USA}, title={Nonlinear Compliant Modes for Large-Deformation Analysis of Flexible Structures}, volume={42}, ISSN={0730-0301}, url={https://doi.org/10.1145/3568952}, DOI={10.1145/3568952}, abstractNote={Many flexible structures are characterized by a small number of compliant modes, i.e., large-deformation paths that can be traversed with little mechanical effort, whereas resistance to other deformations is much stiffer. Predicting the compliant modes for a given flexible structure, however, is challenging. While linear eigenmodes capture the small-deformation behavior, they quickly divert into states of unrealistically high energy for larger displacements. Moreover, they are inherently unable to predict nonlinear phenomena such as buckling, stiffening, multistability, and contact. To address this limitation, we propose Nonlinear Compliant Modesâa physically principled extension of linear eigenmodes for large-deformation analysis. Instead of constraining the entire structure to deform along a given eigenmode, our method only prescribes the projection of the systemâs state onto the linear mode while all other degrees of freedom follow through energy minimization. We evaluate the potential of our method on a diverse set of flexible structures, ranging from compliant mechanisms to topology-optimized joints and structured materials. As validated through experiments on physical prototypes, our method correctly predicts a broad range of nonlinear effects that linear eigenanalysis fails to capture.}, number={2}, journal={ACM Trans. Graph.}, publisher={Association for Computing Machinery}, author={Duenser, Simon and Thomaszewski, Bernhard and Poranne, Roi and Coros, Stelian}, year={2022}, month=nov }

 @article{He_Zhang_Ren_Sun_2015, title={Deep Residual Learning for Image Recognition}, url={http://arxiv.org/abs/1512.03385}, DOI={10.48550/arXiv.1512.03385}, abstractNote={Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.}, note={arXiv:1512.03385 [cs]}, number={arXiv:1512.03385}, publisher={arXiv}, author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian}, year={2015}, month=dec }


 @book{Ogden_1997, title={Non-linear Elastic Deformations}, ISBN={978-0-486-69648-5}, url={https://books.google.it/books?id=2u7wCaojfbEC}, publisher={Dover Publications}, author={Ogden, R. W.}, year={1997} }

@book{Goodfellow-et-al-2016,
  added-at = {2017-03-13T20:27:27.000+0100},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  biburl = {https://www.bibsonomy.org/bibtex/2175f81afff897a68829e4d30c080a8fb/hotho},
  interhash = {62814dec510d5c55b0b38ad85a6c748d},
  intrahash = {175f81afff897a68829e4d30c080a8fb},
  keywords = {book deep learning toread},
  note = {Book in preparation for MIT Press},
  publisher = {MIT Press},
  timestamp = {2017-04-14T13:44:20.000+0200},
  title = {Deep Learning},
  url = {http://www.deeplearningbook.org},
  year = 2016
}

 @inbook{Dedieu_2015, address={Berlin, Heidelberg}, title={Newton-Raphson Method}, ISBN={978-3-540-70529-1}, url={https://doi.org/10.1007/978-3-540-70529-1_374}, DOI={10.1007/978-3-540-70529-1_374}, booktitle={Encyclopedia of Applied and Computational Mathematics}, publisher={Springer Berlin Heidelberg}, author={Dedieu, Jean-Pierre}, editor={Engquist, BjÃ¶rn}, year={2015}, pages={1023â1028} }

@misc{xu2015empiricalevaluationrectifiedactivations,
      title={Empirical Evaluation of Rectified Activations in Convolutional Network}, 
      author={Bing Xu and Naiyan Wang and Tianqi Chen and Mu Li},
      year={2015},
      eprint={1505.00853},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1505.00853}, 
}