

\section{Introduction}
Numerical simulations are fundamental to understand complex physical phenomena across diverse scientific and engineering domains. They allow us to explore system behavior under conditions that may be difficult, expensive, or impossible to replicate experimentally. Finite Element Modeling (FEM) stands out as one of the most widely adopted computational techniques for solving complex Partial Differential Equations (PDEs) by discretizing continuous domains into manageable finite element meshes. Despite its proven effectiveness, FEM comes with a significant computational burden, particularly when high-resolution accuracy is essential for reliable results. This thesis investigates how Deep Learning (DL) can be leveraged to dramatically reduce FEM computational costs while maintaining simulation accuracy, with specific focus on modeling object deformation under external forces.


The relationship between degrees of freedom and simulation quality is central to the FEM trade-off. Full-order FEM models typically require many degrees of freedom (DOFs) to accurately represent complex geometries and physical phenomena; in some cases, thousands or even millions of DOFs are necessary to achieve acceptable results. This high dimensionality translates directly into increased computational cost, both in terms of memory requirements and processing time. Reduced-order models (ROMs) offer an alternative by projecting the full-order problem onto a lower-dimensional subspace, thereby significantly reducing the number of DOFs. Ideally, a ROM can achieve a similar level of accuracy to a full-order model, but with a fraction of the computational cost. The challenge lies in constructing ROMs that are both accurate and efficient, particularly for nonlinear problems where simple linear projections may not suffice. This thesis explores the use of deep learning techniques to enhance the accuracy of ROMs, allowing for high-fidelity simulations with significantly reduced computational demands.

In the surgical field, for example, the ability to simulate soft tissue deformation in real-time is crucial for computer-assisted interventions \cite{petit:hal-01930366}. Surgeons rely on accurate models to predict how tissues will respond to surgical tools, which can vary significantly based on the material properties and the nature of the forces applied. Traditional FEM approaches, while accurate, often struggle with the computational demands of real-time applications, especially in scenarios involving large deformations or complex geometries. The need for faster simulations without sacrificing accuracy has led to the exploration of surrogates to replace or augment traditional FEM methods. 
Our research addresses this challenge by developing a reduced-order modeling approach that combines linear modal analysis with deep learning corrections. Rather than replacing traditional FEM entirely, we propose training neural networks to enhance linear modes, enabling accurate simulations even with coarse discretizations. This hybrid approach aims to deliver the speed advantages of reduced-order models while achieving the accuracy typically associated with high-resolution FEM.


The integration of deep learning into scientific computing has gained a lot of popularity, facilitated by frameworks like TensorFlow \cite{tensorflow2015-whitepaper} and PyTorch \cite{paszke2019pytorchimperativestylehighperformance} and the quick evolution of Graphics Processing Units (GPUs), that in the last decade saw an exponential improvement and helped in solving problem deemed impossible before. Various approaches have emerged in the literature, from end-to-end neural surrogates like MeshGraphNet \cite{pfaffLearningMeshBasedSimulation2021a} and its multiscale variants \cite{fortunatoMultiScaleMeshGraphNets2022}, to physics-constrained approaches exemplified by MeshfreeFlowNet \cite{jiangMeshfreeFlowNetPhysicsConstrainedDeep2020} and mesh-reduced prediction methods \cite{hanPredictingPhysicsMeshreduced2022a}.


This project initially focused on a different approach, taking inspiration from the world of computational fluid dynamics (CFD) and the use of neural networks to accelerate simulations. Starting from a paper that used a neural network to map the velocity field on a coarse mesh to a fine mesh \cite{kienerDatadrivenCorrectionCoarse2023}, we decided to apply the concept to the field of computational solid mechanics, specifically to the simulation of object deformation. We immediately encountered some difficulties, mainly due to the fundamental differences between fluid and solid mechanics, the fact that the latter is simulated with the so-called Lagrangian approach, where the mesh is deformed along with the object, while in fluid mechanics, which uses the Eulerian approach, the mesh remains fixed in space, making it easier to use various techniques, such as convolutional neural networks (CNNs) \cite{journals/corr/OSheaN15}. CNNs are particularly effective for fixed meshes, as they can exploit the spatial structure of the data. We tried to adapt the approach to the Lagrangian framework, with the aid of Graph Neural Networks (GNNs) \cite{kipf2017semisupervisedclassificationgraphconvolutional}, which are designed to work with graph-structured data, such as meshes. However, after working on the problem for a few months, we realized that, while the intuition behind the approach was sound, the idea of correcting a ``quasi'' correct field with a neural network, mapping the coarse mesh to a fine mesh required a lot of computational resources for the offline training phase, and then still required to solve a full-order problem at each time step, albeit with a reduced number of nodes in the mesh. In the end, while working on this project, a new paper was published that kept the idea of correcting the displacement field, but using a reduced order model approach, which was much more efficient and effective \cite{Wang_Du_Coros_Thomaszewski_2024}. This paper inspired us to pivot our project and build upon the idea of neural modes.



This thesis is organized as follows: Section \ref{sec:problem_setting} establishes our mathematical foundation, introducing the Neo-Hookean hyperelastic model for large deformation analysis and reviewing linear modal analysis as a classical dimensionality reduction technique, while acknowledging its limitations for nonlinear systems. Section \ref{sec:methods} presents our core investigation, the ``Neural Modes'' architecture, explaining how neural networks learn nonlinear corrections to linear modes through physics-informed loss functions and how these enhanced modes integrate into dynamic simulations. Finally, Section \ref{sec:numerical_results} validates our approach through comprehensive numerical experiments on 3D benchmark problems.

\section*{State of the Art}

Understanding the current landscape of deep learning applications in physics simulation provides essential context for our contribution. This review examines existing approaches to neural simulation of object deformation, highlighting both achievements and limitations that motivate our work.

Graph neural networks have emerged as a natural choice for mesh-based simulations. MeshGraphNet \cite{pfaffLearningMeshBasedSimulation2021a} pioneered this direction by treating mesh elements as graph nodes and using message passing to predict dynamics across aerodynamics, structural mechanics, and cloth simulation problems. While achieving impressive speedups over traditional solvers, MeshGraphNet faces scalability challenges: as mesh resolution increases, spatially nearby points become increasingly distant in graph space, reducing message passing efficiency. Additionally, prediction errors accumulate during long-horizon rollouts, eventually causing simulations to diverge from physical reality.

Recognizing these limitations, Fortunato et al. \cite{fortunatoMultiScaleMeshGraphNets2022} developed MultiScale MeshGraphNets, demonstrating that accurate high-resolution dynamics can be learned on coarser meshes while introducing hierarchical message passing across multiple resolutions. This work is particularly relevant to our approach as it validates the feasibility of cross-resolution information transfer, though error accumulation remains problematic for extended simulations.

Alternative architectures have explored different trade-offs. MeshfreeFlowNet \cite{jiangMeshfreeFlowNetPhysicsConstrainedDeep2020} abandons mesh constraints entirely, generating continuous spatio-temporal solutions that can be sampled at arbitrary resolutions. By incorporating PDE constraints directly into training and using fully convolutional encoders, this approach achieves remarkable computational efficiency across large GPU clusters, while maintaining flexibility for variable domain sizes, but, as with other CFD-inspired methods, it is primarily designed for fixed meshes and Eulerian frameworks.

Temporal modeling presents another crucial challenge. Han et al. \cite{hanPredictingPhysicsMeshreduced2022a} addressed long-term stability through transformer-style attention mechanisms, creating compact mesh representations that enable efficient temporal modeling. Their encoder-decoder architecture demonstrates phase-stable predictions over extended sequences without requiring training noise, representing a significant advance in applying sequence models to high-dimensional physics problems.

The approaches discussed above typically require extensive datasets of high-resolution simulations for training. Physics-informed neural networks (PINNs) \cite{raissi2024physicsinformedneuralnetworksextensions} offer an alternative by incorporating physical laws directly into the learning process. PINNs train networks to satisfy governing equations by adding PDE residuals to loss functions, often achieving high accuracy with significantly less training data.

Building on this foundation, Djeumou et al. \cite{djeumouNeuralNetworksPhysicsInformed2022} developed frameworks that structure neural networks around known physical constraints, representing vector fields as compositions of known and unknown functions. Their approach yields prediction accuracy improvements of up to two orders of magnitude compared to purely data-driven baselines.

Traditional PINNs struggle with scalability and boundary condition enforcement. Gao et al. \cite{gaoPhysicsinformedGraphNeural2022} addressed these issues through discrete PINN frameworks based on Graph Convolutional Networks. By leveraging variational PDE structures and piecewise polynomial basis functions, they reduce search space dimensionality while strictly enforcing boundary conditions, particularly valuable for irregular geometries where CNN-based approaches falter.

Recent work by Tierz et al. \cite{Tierz_Alfaro_Gonz√°lez_Chinesta_Cueto_2025} advances physics-informed approaches by enforcing thermodynamic principles directly in network architecture. Their local formulation avoids complex global matrix assembly while achieving one to two orders of magnitude accuracy improvements, demonstrating strong generalization even for scenarios substantially different from training data.

Most directly relevant to our work is the neural mode approach by Wang et al. \cite{Wang_Du_Coros_Thomaszewski_2024}, which extends classical linear modal dynamics \cite{Pentland_Williams_1989} to handle large deformations. Their method trains networks to learn nonlinear corrections for each modal coordinate, minimizing system energy while maintaining the computational advantages of subspace dynamics. This approach addresses fundamental limitations of linear modes, the inability to capture large deformations and tendency toward unrealistic results, while preserving interpretability often lacking in deep learning solutions.

Our work builds directly upon this foundation, extending the neural mode framework specifically to Neo-Hookean materials commonly used in organic tissue simulation. The key advantage of this approach lies in its interpretability: since we modify linear modes in physically meaningful ways, network predictions and mode combinations can be directly visualized and understood. This transparency provides confidence in results while facilitating model refinement, a crucial consideration for applications requiring both speed and reliability.
