

\section{Introduction}
Numerical simulations are fundamental to understanding complex physical phenomena across diverse scientific and engineering domains. They allow us to explore system behavior under conditions that may be difficult, expensive, or impossible to replicate experimentally. Finite Element Modeling (FEM) stands out as one of the most widely adopted computational techniques for solving complex Partial Differential Equations (PDEs) by discretizing continuous domains into manageable finite element meshes. Despite its proven effectiveness, FEM comes with a significant computational burden, particularly when high-resolution accuracy is essential for reliable results. This thesis investigates how Deep Learning (DL) can be leveraged to dramatically reduce FEM computational costs while maintaining simulation accuracy, with specific focus on modeling object deformation under external forces.

The relationship between mesh resolution and simulation quality lies at the heart of the FEM trade-off. Fine meshes capture intricate deformation details with high fidelity but demand substantial computational resources, while coarser meshes run faster but sacrifice accuracy. This challenge becomes particularly critical in applications such as biomechanics and medical simulation, where understanding soft tissue behavior, like liver deformation during surgical procedures, requires both precision and speed for practical clinical use.

Our research addresses this challenge by developing a reduced-order modeling approach that combines linear modal analysis with deep learning corrections. Rather than replacing traditional FEM entirely, we propose training neural networks to enhance linear modes, enabling accurate simulations even with coarse discretizations. This hybrid approach aims to deliver the speed advantages of reduced-order models while achieving the accuracy typically associated with high-resolution FEM.

The integration of deep learning into scientific computing has gained a lot of popularity, facilitated by frameworks like TensorFlow \cite{tensorflow2015-whitepaper} and PyTorch \cite{paszke2019pytorchimperativestylehighperformance}. Various approaches have emerged in the literature, from end-to-end neural surrogates like MeshGraphNet \cite{pfaffLearningMeshBasedSimulation2021a} and its multiscale variants \cite{fortunatoMultiScaleMeshGraphNets2022}, to physics-constrained approaches exemplified by MeshfreeFlowNet \cite{jiangMeshfreeFlowNetPhysicsConstrainedDeep2020} and mesh-reduced prediction methods \cite{hanPredictingPhysicsMeshreduced2022a}.

This thesis is organized as follows: Section \ref{sec:problem_setting} establishes our mathematical foundation, introducing the Neo-Hookean hyperelastic model for large deformation analysis and reviewing linear modal analysis as a classical dimensionality reduction technique, while acknowledging its limitations for nonlinear systems. Section \ref{sec:methods} presents our core investigation, the ``Neural Modes'' architecture, explaining how neural networks learn nonlinear corrections to linear modes through physics-informed loss functions and how these enhanced modes integrate into dynamic simulations. Finally, Section \ref{sec:numerical_results} validates our approach through comprehensive numerical experiments on 3D benchmark problems.

\section*{State of the Art}

Understanding the current landscape of deep learning applications in physics simulation provides essential context for our contribution. This review examines existing approaches to neural simulation of object deformation, highlighting both achievements and limitations that motivate our work.

Graph neural networks have emerged as a natural choice for mesh-based simulations. MeshGraphNet \cite{pfaffLearningMeshBasedSimulation2021a} pioneered this direction by treating mesh elements as graph nodes and using message passing to predict dynamics across aerodynamics, structural mechanics, and cloth simulation problems. While achieving impressive speedups over traditional solvers, MeshGraphNet faces scalability challenges: as mesh resolution increases, spatially nearby points become increasingly distant in graph space, reducing message passing efficiency. Additionally, prediction errors accumulate during long-horizon rollouts, eventually causing simulations to diverge from physical reality.

Recognizing these limitations, Fortunato et al. \cite{fortunatoMultiScaleMeshGraphNets2022} developed MultiScale MeshGraphNets, demonstrating that accurate high-resolution dynamics can be learned on coarser meshes while introducing hierarchical message passing across multiple resolutions. This work is particularly relevant to our approach as it validates the feasibility of cross-resolution information transfer, though error accumulation remains problematic for extended simulations.

Alternative architectures have explored different trade-offs. MeshfreeFlowNet \cite{jiangMeshfreeFlowNetPhysicsConstrainedDeep2020} abandons mesh constraints entirely, generating continuous spatio-temporal solutions that can be sampled at arbitrary resolutions. By incorporating PDE constraints directly into training and using fully convolutional encoders, this approach achieves remarkable computational efficiency across large GPU clusters, while maintaining flexibility for variable domain sizes.

Temporal modeling presents another crucial challenge. Han et al. \cite{hanPredictingPhysicsMeshreduced2022a} addressed long-term stability through transformer-style attention mechanisms, creating compact mesh representations that enable efficient temporal modeling. Their encoder-decoder architecture demonstrates phase-stable predictions over extended sequences without requiring training noise, representing a significant advance in applying sequence models to high-dimensional physics problems.

The approaches discussed above typically require extensive datasets of high-resolution simulations for training. Physics-informed neural networks (PINNs) \cite{raissi2024physicsinformedneuralnetworksextensions} offer an alternative by incorporating physical laws directly into the learning process. PINNs train networks to satisfy governing equations by adding PDE residuals to loss functions, often achieving high accuracy with significantly less training data.

Building on this foundation, Djeumou et al. \cite{djeumouNeuralNetworksPhysicsInformed2022} developed frameworks that structure neural networks around known physical constraints, representing vector fields as compositions of known and unknown functions. Their approach yields prediction accuracy improvements of up to two orders of magnitude compared to purely data-driven baselines.

Traditional PINNs struggle with scalability and boundary condition enforcement. Gao et al. \cite{gaoPhysicsinformedGraphNeural2022} addressed these issues through discrete PINN frameworks based on Graph Convolutional Networks. By leveraging variational PDE structures and piecewise polynomial basis functions, they reduce search space dimensionality while strictly enforcing boundary conditions, particularly valuable for irregular geometries where CNN-based approaches falter.

Recent work by Tierz et al. \cite{Tierz_Alfaro_Gonz√°lez_Chinesta_Cueto_2025} advances physics-informed approaches by enforcing thermodynamic principles directly in network architecture. Their local formulation avoids complex global matrix assembly while achieving one to two orders of magnitude accuracy improvements, demonstrating strong generalization even for scenarios substantially different from training data.

Most directly relevant to our work is the neural mode approach by Wang et al. \cite{Wang_Du_Coros_Thomaszewski_2024}, which extends classical linear modal dynamics \cite{Pentland_Williams_1989} to handle large deformations. Their method trains networks to learn nonlinear corrections for each modal coordinate, minimizing system energy while maintaining the computational advantages of subspace dynamics. This approach addresses fundamental limitations of linear modes, the inability to capture large deformations and tendency toward unrealistic results, while preserving interpretability often lacking in deep learning solutions.

Our work builds directly upon this foundation, extending the neural mode framework specifically to Neo-Hookean materials commonly used in organic tissue simulation. The key advantage of this approach lies in its interpretability: since we modify linear modes in physically meaningful ways, network predictions and mode combinations can be directly visualized and understood. This transparency provides confidence in results while facilitating model refinement, a crucial consideration for applications requiring both speed and reliability.
