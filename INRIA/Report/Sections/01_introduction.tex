\section{Introduction}

Numerical simulations play a critical role in a wide array of scientific and engineering applications, providing insights into the behavior of physical systems under various conditions. Among the most prominent techniques for performing such simulations is Finite Element Modeling (FEM). FEM discretizes a continuous domain into a mesh of finite elements, allowing for the approximation of solutions to complex partial differential equations (PDEs). However, one significant drawback of FEM is its computational intensity, especially when high resolution is required for accurate results. This research aims to explore the potential of Deep Learning (DL) techniques to accelerate FEM simulations, focusing specifically on the deformation of objects subjected to external forces.

The deformation of an object under an applied force is directly tied to the object's discretization. In FEM, the object is represented by a mesh, where the resolution of the mesh—i.e., the size and number of elements—clearly impacts the accuracy and computational cost of the simulation. High-resolution meshes can capture fine details of deformation, leading to more accurate simulations, but they are computationally expensive and time-consuming. 

The main goal of this work is to study the efficacy of a method that combines both Finite Element Modeling (FEM) and DL to obtain a realistic simulation of an object in a fraction of the time that would be required by a traditional FEM simulation. The idea is to, somehow, train a DL model to have inside the information given by the refined discretization and pass them on a coarser discretization.

The idea of using DL techniques to solve scientific problem is not new. Thanks to the rise of new frameworks and libraries, such as TensorFlow and PyTorch, it is now possible to train very complex models on large datasets in a reasonable amount of time. For the problem at hand, a lot of different approaches can be found in the existing literature: a lot of them are based on the idea that the deep learning model should predict the whole dynamic of the system, for example MeshGraphNet \cite{pfaffLearningMeshBasedSimulation2021a} or its multiscale version \cite{fortunatoMultiScaleMeshGraphNets2022}, but these are just two examples of the many possible approaches \cite{jiangMeshfreeFlowNetPhysicsConstrainedDeep2020}, \cite{djeumouNeuralNetworksPhysicsInformed2022}, \cite{hanPredictingPhysicsMeshreduced2022a}. Other methods rely on solving a time independent problem, using various architectures, such as PINNs \cite{djeumouNeuralNetworksPhysicsInformed2022} or GNNs \cite{gaoPhysicsinformedGraphNeural2022}. The proposed method falls into the second category, as it will be explained in the following sections.

The inspiration for this work came from the world of Computational Fluid Dynamics (CFD), particularly from a paper in which the authors propose a data-driven correction to the solution of a coarse grid simulation, using a neural network trained on the fine grid simulation \cite{kienerDatadrivenCorrectionCoarse2023}. The main concept is to create two grids on a fixed domain in a way that the fine grid is a refinement of the coarse grid. In this way, by means of an interpolation operator, it is possible to have both solutions on the same grid and perform computations with them, such as the difference between the two solution or between the derivatives of the solution. The neural network is trained to predict the difference between the fine grid solution and the coarse grid solution, given the coarse grid solution as input. In the paper, they propose various machine learning models, such as a simple feedforward neural network, a random forest and a GNN, both in two and three dimensions. The results show that the neural network is able to predict the difference between the two solutions with a good accuracy. 

This work aims at finding a solution to the problem of accelerating FEM simulations in the context of solid mechanics. To achieve such a goal, a good method should have certain characteristics: \begin{itemize}
    \item It should be geometry independent, removing the need for a new training for each new geometry.ù
    \item Should at least be topology independent, meaning that the method should be able to work with different meshes.
    \item Avoid the ``black-box'' problem of having a model predict the whole system without having any insights on what's happening inside.
    \item It should be really fast, to be able to be used in real-time applications.
\end{itemize}
Of those, the first one is clearly the most difficult to achieve, as encoding a geometry is still an open problem. Some progress were made in the field CITARE GINO, but it's far from solved. Also, for the uses of this method, it is assumed that the geometry is known, so this problem is not addressed here. 

The other three points are addressed here. Let's start by the independence with respect to the topology. In the original paper, the authors found a way to superimpose the two grids, but that would imply that the two meshes must be created together, limiting the possibilities. In this case it is possible to take advantage of the FEM itself, allowing to compute the exact solution everywhere on the domain, and then interpolate it on the mutual grid. By following this approach, it is possible to have a method that is independent of the topology of the mesh, since the position of the points on the grid is always the same with respect to the domain.

The main problem with ``black-box'' models is that they are not interpretable, so if the model isn't working as expected, it is difficult to understand why. One example is the MeshGraphNet, which is a very complex model that tries to fully predict the dynamic of the system. If the prediction is accumulating errors, one must retrain the model from scratch, trying to understand what went wrong. In this case, the method is based on correcting a numerical solution, so the starting point is always the exact solution which the model is trained to correct, so theoretically should be easier to understand the weaknesses of the model.

Finally, the speed of the method is a critical point. The method should be able to be used in real-time applications, so it should be as fast as possible. The method proposed here is based on a neural network, which means that instead of solving a linear system of equations, the solution is obtained by a forward pass of the network, which basically consists of a series of matrix multiplications and non-linear functions. This is a very fast operation, especially if the network is small, so the method should be able to be used in real-time applications.

The rest of the report is organized as follows: Section \ref{sec:problem_setting} introduces the problem setting and the mathematical formulation of the problem. Section \ref{sec:neural_network} gives a brief overview of the neural network architectures used in this work. Section \ref{sec:numerical_results} presents the numerical results obtained by the proposed method on selected tests. Finally, Section \ref{sec:conclusions} summarizes the main findings and outlines possible future research directions.