\section{Data driven correction}
\label{sec:dd_correction}

In this section will be presented the idea behind the method proposed to solve this problem. It is something that it's been already explored in the CFD world, although with some differences. 

It will be taken into account two discretization of the domain \(\Omega\), called \(\Omega_c\) and \(\Omega_f\) where the subscript \(c\) stands for ``coarse'' and the subscript \(f\) stands for ``fine''. The final goal is to have a solution on the coarse grid that is as close as possible to the solution on the fine grid. One possibility is to find a map \(G: \Omega_f \rightarrow \Omega_c\) that maps the solution on the fine grid to the coarse grid. 

The first solution that comes to mind is to use a neural network to find this map. So find an approximation \(\mathcal{NN}(\bm{u}) \approx G(\bm{u})\), by training the neural network on a dataset of pairs of solutions on the fine and coarse grid. This method has some drawbacks, the main one is that the neural network is trained on a fixed set of nodes, so if the coarse discretization changes, the method is not working anymore. The ideal solution for this problem is to have a model that is able to compute the solution for any given topology of the grid.

The problem with varying topologies is that the number of nodes of the mesh is not fixed, and this is a problem with Fully Connected Neural Networks (FCNN), where the input size is fixed. The need of a fixed input size can be solved by creating a fixed grid on each geometry, and then interpolate the solution on this grid. At this point, both the solutions on the fine and coarse grid are computed on the same grid. Having the solution on the same grid allows for multiple new possibilities, like computing the difference between the two solutions, a sort of ``error'' that is generated by the coarse grid. A neural network can be trained to learn this error and then correct the solution on the coarse grid, eliminating completely the need of the fine grid. In this way, the simulation can be run on the coarse grid, and then the neural network can be used to correct the solution, avoiding the ``black-box'' approach of having the model predicting a complete solution on a fine grid. This allows the method to be used also in a dynamical setting, avoiding the main problem that affects some existing methods. When a method tries to predict the temporal evolution of the system with deep learning techniques usually present the problem of accumulation of error, because the error at time-step \(t\) is carried over at time-step \(t+1\), and for a large number of time-steps this becomes noticeable and the prediction of the model is not accurate. 

This corrective method instead relies on the numerical solver for the temporal evolution of the problem, minimizing the accumulation of the error since it uses an exact method for the temporal evolution and the ``initial'' prediction before the correction from the model.

In a more formal way, the method can be described as follows. Let \(G \subset \Omega\) be a collection of points placed on a regular grid inside the domain \(\Omega\). Let \(\bm{u}_f\) be the solution of the problem on the fine grid and \(\bm{u}_c\) be the solution of the problem on the coarse grid. Once the two solution are computed, it is possible to take advantage of the FEM to interpolate the solution on the grid \(G\), obtaining \(\bm{u}_f^G\) and \(\bm{u}_c^G\). Now that both solutions live on the same grid, it is possible to compute the difference between the two solutions, obtaining the error \(\bm{e}^G = \bm{u}_f^G - \bm{u}_c^G\). The model is trained on the dataset \(\mathcal{D} = \{(\bm{u}_c^G, \bm{e}^G)\}\) to learn the error and then correct the solution on the coarse grid. The corrected solution is given by 
\[
    \hat{\bm{u}}_f^G = \bm{u}_c^G + \mathcal{NN}(\bm{u}_c^G).
\]
Once the model is trained, it can be used to correct the solution on the coarse grid, by interpolating back the solution on the original grid using Radial Basis Functions (RBF) interpolation. 