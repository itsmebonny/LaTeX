\relax 
\providecommand{\transparent@use}[1]{}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{plain}
\citation{Zhang_2019}
\citation{Abdar_2021}
\citation{Lagaris_1998}
\transparent@use{.4}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Neural Networks}{1}{subsection.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Uncertainty Quantification}{1}{subsection.1.2}\protected@file@percent }
\citation{Zhang_2019}
\citation{Zhang_2019}
\@writefile{toc}{\contentsline {section}{\numberline {2}Problem setup}{2}{section.2}\protected@file@percent }
\newlabel{problem_setup}{{1}{2}{Problem setup}{equation.2.1}{}}
\newlabel{problem_setup@cref}{{[equation][1][]1}{[1][2][]2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Methodology}{2}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}PINNs for deterministic systems}{2}{subsection.3.1}\protected@file@percent }
\newlabel{problem_setup_det}{{2}{2}{PINNs for deterministic systems}{equation.3.2}{}}
\newlabel{problem_setup_det@cref}{{[equation][2][]2}{[1][2][]2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Schematic of a PINN for solving differential equations. Figure from \cite  {Zhang_2019}.\relax }}{2}{figure.caption.3}\protected@file@percent }
\newlabel{second_network}{{3}{3}{PINNs for deterministic systems}{equation.3.3}{}}
\newlabel{second_network@cref}{{[equation][3][]3}{[1][2][]3}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces PINN for solving differential equations\relax }}{3}{algorithm.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{alg:cap}{{1}{3}{PINN for solving differential equations\relax }{algorithm.1}{}}
\newlabel{alg:cap@cref}{{[algorithm][1][]1}{[1][2][]3}}
\newlabel{loss_fn}{{4}{3}{PINN for solving differential equations\relax }{equation.3.4}{}}
\newlabel{loss_fn@cref}{{[equation][4][]4}{[1][2][]3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Moving to a stochastic setting}{3}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Dimension reduction with PCA}{3}{subsubsection.3.2.1}\protected@file@percent }
\citation{Zhang_2019}
\citation{Zhang_2019}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Arbitrary polynomial chaos expansion}{4}{subsubsection.3.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3}Learning stochastic modes}{4}{subsubsection.3.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Schematic of the various networks needed for learning the stochastic modes. There are two smaller networks which calculates the two means and the bigger networks calculate the rest of the modes. Figure from \cite  {Zhang_2019}.\relax }}{4}{figure.caption.6}\protected@file@percent }
\citation{Zhang_2019}
\citation{Zhang_2019}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.4}Dropout}{5}{subsubsection.3.2.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces An example of using dropout for quantify uncertainty. Here the function \(y = x^3e^{-x}\) is approximated using a NN. Mean and standard deviation are calculated from \(1000\) MC runs. Figure from \cite  {Zhang_2019}.\relax }}{5}{figure.caption.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Numerical Results}{5}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Solving SPDEs}{5}{subsection.4.1}\protected@file@percent }
\newlabel{inverse_pbl}{{5}{5}{Solving SPDEs}{equation.4.5}{}}
\newlabel{inverse_pbl@cref}{{[equation][5][]5}{[1][5][]5}}
\citation{Zhang_2019}
\citation{Zhang_2019}
\citation{Zhang_2019}
\citation{Zhang_2019}
\citation{Zhang_2019}
\citation{Zhang_2019}
\citation{Zhang_2019}
\citation{Zhang_2019}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Comparing the prediction accuracy against various combinations of networks and values of the L2 regularizer \(\lambda \) (left side). Comparing the prediction accuracy against the number of training points (sensors) for \(u\) and \(k\). Figure from \cite  {Zhang_2019}.\relax }}{6}{figure.caption.9}\protected@file@percent }
\newlabel{fig:regularization}{{4}{6}{Comparing the prediction accuracy against various combinations of networks and values of the L2 regularizer \(\lambda \) (left side). Comparing the prediction accuracy against the number of training points (sensors) for \(u\) and \(k\). Figure from \cite {Zhang_2019}.\relax }{figure.caption.9}{}}
\newlabel{fig:regularization@cref}{{[figure][4][]4}{[1][6][]6}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Comparing the L2 error when using \(1^{st}\) and \(2^{nd}\) order expansion aPC. Figure from \cite  {Zhang_2019}.\relax }}{6}{figure.caption.10}\protected@file@percent }
\newlabel{fig:table}{{5}{6}{Comparing the L2 error when using \(1^{st}\) and \(2^{nd}\) order expansion aPC. Figure from \cite {Zhang_2019}.\relax }{figure.caption.10}{}}
\newlabel{fig:table@cref}{{[figure][5][]5}{[1][6][]6}}
\citation{Zhang_2019}
\citation{Zhang_2019}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Predicted mean and standard deviation of \(u\) for various order expansions. Figure from \cite  {Zhang_2019}.\relax }}{7}{figure.caption.11}\protected@file@percent }
\newlabel{fig:mean_u}{{6}{7}{Predicted mean and standard deviation of \(u\) for various order expansions. Figure from \cite {Zhang_2019}.\relax }{figure.caption.11}{}}
\newlabel{fig:mean_u@cref}{{[figure][6][]6}{[1][6][]7}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Predicted mean and standard deviation of \(k\) for various order expansion. Figure from \cite  {Zhang_2019}.\relax }}{7}{figure.caption.12}\protected@file@percent }
\newlabel{fig:mean_k}{{7}{7}{Predicted mean and standard deviation of \(k\) for various order expansion. Figure from \cite {Zhang_2019}.\relax }{figure.caption.12}{}}
\newlabel{fig:mean_k@cref}{{[figure][7][]7}{[1][6][]7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Solving deterministic differential equations with PINNs and dropout}{7}{subsection.4.2}\protected@file@percent }
\newlabel{pbl:deterministic}{{6}{7}{Solving deterministic differential equations with PINNs and dropout}{equation.4.6}{}}
\newlabel{pbl:deterministic@cref}{{[equation][6][]6}{[1][7][]7}}
\citation{Zhang_2019}
\citation{Zhang_2019}
\citation{Zhang_2019}
\citation{Zhang_2019}
\citation{Zhang_2019}
\citation{Zhang_2019}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Prediction of \(k(x)\) with and without dropout. Figure from \cite  {Zhang_2019}.\relax }}{8}{figure.caption.14}\protected@file@percent }
\newlabel{fig:drop_k}{{8}{8}{Prediction of \(k(x)\) with and without dropout. Figure from \cite {Zhang_2019}.\relax }{figure.caption.14}{}}
\newlabel{fig:drop_k@cref}{{[figure][8][]8}{[1][7][]8}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Active learning for inverse stochastic problems}{8}{subsubsection.4.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Prediction of \(k\) at the first step (left) and at the step \(15\) (right). Figure from \cite  {Zhang_2019}.\relax }}{8}{figure.caption.15}\protected@file@percent }
\newlabel{fig:act_k}{{9}{8}{Prediction of \(k\) at the first step (left) and at the step \(15\) (right). Figure from \cite {Zhang_2019}.\relax }{figure.caption.15}{}}
\newlabel{fig:act_k@cref}{{[figure][9][]9}{[1][8][]8}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Error reduction in \(15\) steps. Figure from \cite  {Zhang_2019}.\relax }}{8}{figure.caption.16}\protected@file@percent }
\newlabel{fig:act2_k}{{10}{8}{Error reduction in \(15\) steps. Figure from \cite {Zhang_2019}.\relax }{figure.caption.16}{}}
\newlabel{fig:act2_k@cref}{{[figure][10][]10}{[1][8][]8}}
\bibdata{bibliography.bib}
\bibcite{Abdar_2021}{{1}{}{{}}{{}}}
\bibcite{Lagaris_1998}{{2}{}{{}}{{}}}
\bibcite{Zhang_2019}{{3}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Predictions of \(k\) (left) and \(u\) (right) at some steps, along with the new training points. Figure from \cite  {Zhang_2019}.\relax }}{9}{figure.caption.17}\protected@file@percent }
\newlabel{fig:act3_k}{{11}{9}{Predictions of \(k\) (left) and \(u\) (right) at some steps, along with the new training points. Figure from \cite {Zhang_2019}.\relax }{figure.caption.17}{}}
\newlabel{fig:act3_k@cref}{{[figure][11][]11}{[1][8][]9}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusions}{9}{section.5}\protected@file@percent }
\gdef \@abspage@last{9}
