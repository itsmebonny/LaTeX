\relax 
\providecommand{\transparent@use}[1]{}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{plain}
\citation{Zhang_2019}
\citation{Abdar_2021}
\citation{Lagaris_1998}
\transparent@use{.4}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Neural Networks}{1}{subsection.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Uncertainty Quantification}{1}{subsection.1.2}\protected@file@percent }
\citation{Zhang_2019}
\citation{Zhang_2019}
\@writefile{toc}{\contentsline {section}{\numberline {2}Problem setup}{2}{section.2}\protected@file@percent }
\newlabel{problem_setup}{{1}{2}{Problem setup}{equation.2.1}{}}
\newlabel{problem_setup@cref}{{[equation][1][]1}{[1][2][]2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Methodology}{2}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}PINNs for deterministic systems}{2}{subsection.3.1}\protected@file@percent }
\newlabel{problem_setup_det}{{2}{2}{PINNs for deterministic systems}{equation.3.2}{}}
\newlabel{problem_setup_det@cref}{{[equation][2][]2}{[1][2][]2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Schematic of a PINN for solving differential equations. Figure from \cite  {Zhang_2019}.\relax }}{2}{figure.caption.3}\protected@file@percent }
\newlabel{second_network}{{3}{2}{PINNs for deterministic systems}{equation.3.3}{}}
\newlabel{second_network@cref}{{[equation][3][]3}{[1][2][]2}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces PINN for solving differential equations\relax }}{3}{algorithm.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{alg:cap}{{1}{3}{PINN for solving differential equations\relax }{algorithm.1}{}}
\newlabel{alg:cap@cref}{{[algorithm][1][]1}{[1][2][]3}}
\newlabel{loss_fn}{{4}{3}{PINN for solving differential equations\relax }{equation.3.4}{}}
\newlabel{loss_fn@cref}{{[equation][4][]4}{[1][2][]3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Moving to a stochastic setting}{3}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Dimension reduction with PCA}{3}{subsubsection.3.2.1}\protected@file@percent }
\citation{Zhang_2019}
\citation{Zhang_2019}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Arbitrary polynomial chaos expansion}{4}{subsubsection.3.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3}Learning stochastic modes}{4}{subsubsection.3.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Schematic of the various networks needed for learning the stochastic modes. There are two smaller networks which calculates the two means and the bigger networks calculate the rest of the modes. Figure from \cite  {Zhang_2019}.\relax }}{4}{figure.caption.6}\protected@file@percent }
\citation{Zhang_2019}
\citation{Zhang_2019}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.4}Dropout}{5}{subsubsection.3.2.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces An example of using dropout for quantify uncertainty. Here the function \(y = x^3e^{-x}\) is approximated using a NN. Mean and standard deviation are calculated from \(1000\) MC runs. Figure from \cite  {Zhang_2019}.\relax }}{5}{figure.caption.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Numerical Results}{5}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Solving SPDEs}{5}{subsection.4.1}\protected@file@percent }
\newlabel{inverse_pbl}{{5}{5}{Solving SPDEs}{equation.4.5}{}}
\newlabel{inverse_pbl@cref}{{[equation][5][]5}{[1][5][]5}}
\citation{Zhang_2019}
\citation{Zhang_2019}
\citation{Zhang_2019}
\citation{Zhang_2019}
\citation{Zhang_2019}
\citation{Zhang_2019}
\citation{Zhang_2019}
\citation{Zhang_2019}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Comparing the prediction accuracy against various combinations of networks and values of the L2 regularizer \(\lambda \) (left side). Comparing the prediction accuracy against the number of training points (sensors) for \(u\) and \(k\). Figure from \cite  {Zhang_2019}.\relax }}{6}{figure.caption.9}\protected@file@percent }
\newlabel{fig:regularization}{{4}{6}{Comparing the prediction accuracy against various combinations of networks and values of the L2 regularizer \(\lambda \) (left side). Comparing the prediction accuracy against the number of training points (sensors) for \(u\) and \(k\). Figure from \cite {Zhang_2019}.\relax }{figure.caption.9}{}}
\newlabel{fig:regularization@cref}{{[figure][4][]4}{[1][6][]6}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Comparing the L2 error when using \(1^{st}\) and \(2^{nd}\) order expansion aPC. Figure from \cite  {Zhang_2019}.\relax }}{6}{figure.caption.10}\protected@file@percent }
\newlabel{fig:table}{{5}{6}{Comparing the L2 error when using \(1^{st}\) and \(2^{nd}\) order expansion aPC. Figure from \cite {Zhang_2019}.\relax }{figure.caption.10}{}}
\newlabel{fig:table@cref}{{[figure][5][]5}{[1][6][]6}}
\bibdata{bibliography.bib}
\bibcite{Abdar_2021}{{1}{}{{}}{{}}}
\bibcite{Lagaris_1998}{{2}{}{{}}{{}}}
\bibcite{Zhang_2019}{{3}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Predicted mean and standard deviation of \(u\) for various order expansions. Figure from \cite  {Zhang_2019}.\relax }}{7}{figure.caption.11}\protected@file@percent }
\newlabel{fig:mean_u}{{6}{7}{Predicted mean and standard deviation of \(u\) for various order expansions. Figure from \cite {Zhang_2019}.\relax }{figure.caption.11}{}}
\newlabel{fig:mean_u@cref}{{[figure][6][]6}{[1][6][]7}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Predicted mean and standard deviation of \(k\) for various order expansion. Figure from \cite  {Zhang_2019}.\relax }}{7}{figure.caption.12}\protected@file@percent }
\newlabel{fig:mean_k}{{7}{7}{Predicted mean and standard deviation of \(k\) for various order expansion. Figure from \cite {Zhang_2019}.\relax }{figure.caption.12}{}}
\newlabel{fig:mean_k@cref}{{[figure][7][]7}{[1][6][]7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Solving deterministic differential equations with PINNs and dropout}{7}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusions}{7}{section.5}\protected@file@percent }
\gdef \@abspage@last{7}
